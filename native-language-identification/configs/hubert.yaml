# HuBERT-specific Configuration

# Inherits from default.yaml and overrides specific settings

experiment_name: "hubert_layerwise_analysis"

# Feature Extraction - HuBERT focused
features:
  use_mfcc: false
  use_hubert: true
  
  hubert:
    model_name: "facebook/hubert-base-ls960"
    # Layer-wise analysis: extract from multiple layers
    extract_layers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
    pooling: "mean"  # Options: mean, max, attention, cls_token
    normalize: true
    freeze_encoder: false  # Set to true for feature extraction only
    
    # Fine-tuning settings
    finetune:
      enabled: true
      unfreeze_layers: 2  # Number of top layers to unfreeze
      learning_rate: 0.00001
      warmup_steps: 500

# Model Architecture - optimized for HuBERT embeddings
model:
  architecture: "transformer"  # Recommended for HuBERT
  
  transformer:
    num_heads: 8
    ff_dim: 512
    num_layers: 3
    dropout: 0.1
    dense_units: 256
    use_positional_encoding: true

# Training Settings - adjusted for HuBERT
training:
  batch_size: 16  # Smaller batch size due to larger model
  epochs: 50
  learning_rate: 0.0001
  optimizer: "adamw"
  weight_decay: 0.01
  
  # Gradient settings
  gradient_clip_norm: 1.0
  accumulation_steps: 2  # Effective batch size = 16 * 2 = 32
  
  lr_schedule:
    type: "cosine_with_warmup"
    warmup_epochs: 5
    min_lr: 0.000001

# Layer-wise Analysis Settings
layer_analysis:
  enabled: true
  # Train separate classifiers on each layer's output
  train_per_layer: true
  # Compare layer performance
  layers_to_compare: [1, 3, 6, 9, 12]
  save_layer_outputs: true
  output_dir: "./experiments/hubert_representation/layer_analysis"

# Hardware Settings - HuBERT requires more resources
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true
  cache_features: true  # Cache extracted features to speed up training

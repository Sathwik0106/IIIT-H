# Native Language Identification of Indian English Speakers Using HuBERT

## ğŸ“‹ Project Overview

This project develops a system to identify the native language (L1) of Indian speakers speaking English by analyzing accent patterns in their speech. The system leverages both traditional acoustic features (MFCCs) and self-supervised representations (HuBERT embeddings) to classify speakers' native languages.

### Key Features

- **Multi-Feature Approach**: Compares MFCC and HuBERT embeddings for accent detection
- **Layer-wise Analysis**: Analyzes which HuBERT layers best capture accent information
- **Multiple Architectures**: Supports CNN, BiLSTM, and Transformer models
- **Cross-Age Generalization**: Tests model performance across adult and child speakers
- **Multi-Level Analysis**: Compares word-level vs. sentence-level accent detection
- **Real-World Application**: Accent-aware cuisine recommendation system for restaurants

### Supported Languages

- Hindi
- Tamil
- Telugu
- Malayalam
- Kannada
- Bengali
- Gujarati
- Marathi

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd native-language-identification

# Install dependencies
pip install -r requirements.txt

# Optional: Install in development mode
pip install -e .
```

### Dataset

The project uses the [IndicAccentDb](https://huggingface.co/datasets/DarshanaS/IndicAccentDb) dataset from HuggingFace, which contains English speech recordings from Indian speakers with various native language backgrounds.

### Basic Usage

```python
from src.data import IndicAccentDataLoader, AudioPreprocessor
from src.features import MFCCExtractor, HuBERTFeatureExtractor
from src.models import create_classifier
from src.utils import load_config

# Load configuration
config = load_config('configs/default.yaml')

# Load dataset
loader = IndicAccentDataLoader()
dataset = loader.load_dataset()
splits = loader.create_splits()

# Extract features and train model
# See notebooks for detailed examples
```

## ğŸ“ Project Structure

```
native-language-identification/
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ default.yaml         # Default configuration
â”‚   â”œâ”€â”€ hubert.yaml          # HuBERT-specific config
â”‚   â””â”€â”€ mfcc_baseline.yaml   # MFCC baseline config
â”œâ”€â”€ data/                     # Data directory
â”‚   â”œâ”€â”€ raw/                 # Raw dataset
â”‚   â”œâ”€â”€ processed/           # Processed features
â”‚   â””â”€â”€ metadata/            # Dataset metadata
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ data/                # Data loading and preprocessing
â”‚   â”œâ”€â”€ features/            # Feature extraction
â”‚   â”œâ”€â”€ models/              # Model architectures
â”‚   â”œâ”€â”€ training/            # Training utilities
â”‚   â”œâ”€â”€ evaluation/          # Evaluation metrics
â”‚   â”œâ”€â”€ application/         # Application code
â”‚   â””â”€â”€ utils/               # Utility functions
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”œâ”€â”€ experiments/             # Experiment results
â”œâ”€â”€ scripts/                 # Shell scripts
â”œâ”€â”€ models/                  # Saved models
â”‚   â””â”€â”€ checkpoints/        # Model checkpoints
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ tests/                   # Unit tests
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ setup.py                # Package setup
â””â”€â”€ README.md               # This file
```

## ğŸ”¬ Experiments

### 1. MFCC Baseline

```bash
python scripts/train.sh --config configs/mfcc_baseline.yaml
```

### 2. HuBERT Layer-wise Analysis

```bash
python scripts/train.sh --config configs/hubert.yaml
```

### 3. Cross-Age Generalization

Train on adults, test on children to analyze age generalization.

### 4. Word vs. Sentence Level

Compare accent detection performance at different linguistic levels.

## ğŸ“Š Model Architectures

### CNN Classifier
- Convolutional layers for temporal pattern extraction
- Max pooling for feature aggregation
- Fully connected layers for classification

### BiLSTM Classifier
- Bidirectional LSTM for sequence modeling
- Captures temporal dependencies in speech
- Dense layers for final classification

### Transformer Classifier
- Multi-head self-attention mechanism
- Positional encoding for sequence information
- Feedforward networks for transformation

## ğŸ½ï¸ Application: Accent-Aware Cuisine Recommender

The system includes a real-world application that recommends regional Indian cuisines based on detected accents:

```python
from src.application import CuisineRecommender, RestaurantApplication

# Initialize recommender
recommender = CuisineRecommender(
    model=trained_model,
    label_mapping=label_map,
    cuisine_mapping=cuisine_map,
    preprocessor=preprocessor,
    feature_extractor=feature_extractor
)

# Get recommendations
audio = load_customer_speech()
recommendations = recommender.recommend_cuisines(audio, top_k=3)
print(recommendations['recommended_cuisines'])
```

### Example Recommendations

- **Malayalam accent** â†’ Appam, Puttu, Avial, Fish Moilee
- **Hindi accent** â†’ Butter Chicken, Chole Bhature, Dal Makhani
- **Tamil accent** â†’ Dosa, Idli, Sambar, Chettinad Chicken

## ğŸ“ˆ Performance Metrics

The system evaluates models using:
- Accuracy
- Precision, Recall, F1-Score (weighted and per-class)
- Confusion Matrix
- Layer-wise separability analysis (for HuBERT)

## ğŸ”§ Configuration

Edit `configs/default.yaml` to customize:
- Model architecture and hyperparameters
- Feature extraction settings
- Training parameters
- Data split ratios
- Evaluation metrics

## ğŸ“ Notebooks

1. `01_data_exploration.ipynb` - Dataset analysis and visualization
2. `02_feature_extraction.ipynb` - MFCC and HuBERT feature extraction
3. `03_training_and_eval.ipynb` - Model training and evaluation

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/

# Run specific test
pytest tests/test_data.py
```

## ğŸ“š Documentation

For detailed information about the dataset and experimental protocol, see `docs/dataset_and_protocol.md`.

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Dataset: [IndicAccentDb](https://huggingface.co/datasets/DarshanaS/IndicAccentDb)
- HuBERT Model: [Facebook AI Research](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert)
- Transformers Library: [Hugging Face](https://huggingface.co/transformers/)

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

## ğŸ¯ Future Work

- Extend to more Indian languages
- Real-time accent detection
- Mobile application deployment
- Multi-modal analysis (audio + text)
- Cross-lingual accent transfer studies

---

**Note**: This is an academic/research project for native language identification. Ensure ethical use and respect for linguistic diversity.

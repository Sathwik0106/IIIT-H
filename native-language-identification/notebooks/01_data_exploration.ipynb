{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d66cfa3",
   "metadata": {},
   "source": [
    "# Native Language Identification - Data Exploration\n",
    "\n",
    "This notebook explores the IndicAccentDb dataset for native language identification of Indian English speakers.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and explore the dataset\n",
    "2. Analyze language distribution\n",
    "3. Visualize audio samples\n",
    "4. Extract and visualize acoustic features\n",
    "5. Prepare data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb56366",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "# Project modules\n",
    "from src.data import IndicAccentDataLoader, AudioPreprocessor\n",
    "from src.features import MFCCExtractor\n",
    "from src.utils import load_config\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0140b",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../configs/default.yaml')\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Project: {config['project_name']}\")\n",
    "print(f\"Dataset: {config['data']['dataset_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = IndicAccentDataLoader(\n",
    "    dataset_name=config['data']['dataset_name'],\n",
    "    cache_dir='../data/raw',\n",
    "    processed_dir='../data/processed',\n",
    "    metadata_dir='../data/metadata'\n",
    ")\n",
    "\n",
    "print(\"Data loader initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75037470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (this may take a few minutes)\n",
    "print(\"Loading dataset from HuggingFace...\")\n",
    "dataset = loader.load_dataset()\n",
    "print(f\"Dataset loaded: {dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0023a",
   "metadata": {},
   "source": [
    "### Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15998662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset statistics\n",
    "stats = loader.get_statistics()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Samples: {stats['total_samples']}\")\n",
    "print(f\"Number of Speakers: {stats.get('num_speakers', 'N/A')}\")\n",
    "print(\"\\nLanguage Distribution:\")\n",
    "for lang, count in stats.get('language_distribution', {}).items():\n",
    "    print(f\"  {lang}: {count}\")\n",
    "\n",
    "if 'age_group_distribution' in stats:\n",
    "    print(\"\\nAge Group Distribution:\")\n",
    "    for age, count in stats['age_group_distribution'].items():\n",
    "        print(f\"  {age}: {count}\")\n",
    "\n",
    "if 'duration_stats' in stats:\n",
    "    print(\"\\nAudio Duration Statistics:\")\n",
    "    print(f\"  Mean: {stats['duration_stats']['mean']:.2f}s\")\n",
    "    print(f\"  Std:  {stats['duration_stats']['std']:.2f}s\")\n",
    "    print(f\"  Min:  {stats['duration_stats']['min']:.2f}s\")\n",
    "    print(f\"  Max:  {stats['duration_stats']['max']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb84e8c",
   "metadata": {},
   "source": [
    "## 3. Visualize Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91543fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot language distribution\n",
    "if 'language_distribution' in stats:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    languages = list(stats['language_distribution'].keys())\n",
    "    counts = list(stats['language_distribution'].values())\n",
    "    \n",
    "    ax.bar(languages, counts, color='steelblue', alpha=0.8)\n",
    "    ax.set_xlabel('Native Language', fontsize=12)\n",
    "    ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "    ax.set_title('Distribution of Native Languages in Dataset', fontsize=14, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(i, count, str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot age group and speech level distribution if available\n",
    "if 'age_group_distribution' in stats or 'speech_level_distribution' in stats:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    if 'age_group_distribution' in stats:\n",
    "        ages = list(stats['age_group_distribution'].keys())\n",
    "        age_counts = list(stats['age_group_distribution'].values())\n",
    "        axes[0].pie(age_counts, labels=ages, autopct='%1.1f%%', startangle=90)\n",
    "        axes[0].set_title('Age Group Distribution')\n",
    "    \n",
    "    if 'speech_level_distribution' in stats:\n",
    "        levels = list(stats['speech_level_distribution'].keys())\n",
    "        level_counts = list(stats['speech_level_distribution'].values())\n",
    "        axes[1].pie(level_counts, labels=levels, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1].set_title('Speech Level Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf34313",
   "metadata": {},
   "source": [
    "## 4. Audio Sample Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize audio sample\n",
    "def plot_audio_analysis(audio, sr, title=\"Audio Analysis\"):\n",
    "    \"\"\"Plot waveform, spectrogram, and mel-spectrogram.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Waveform\n",
    "    time = np.arange(len(audio)) / sr\n",
    "    axes[0].plot(time, audio)\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    axes[0].set_title(f'{title} - Waveform')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    img1 = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[1])\n",
    "    axes[1].set_title(f'{title} - Spectrogram')\n",
    "    fig.colorbar(img1, ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    # Mel-spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    img2 = librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[2])\n",
    "    axes[2].set_title(f'{title} - Mel Spectrogram')\n",
    "    fig.colorbar(img2, ax=axes[2], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Audio visualization function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample from each language (if dataset is loaded)\n",
    "# Note: This assumes dataset has audio and language labels\n",
    "# Modify according to actual dataset structure\n",
    "\n",
    "print(\"To visualize audio samples, run the following after loading specific samples:\")\n",
    "print(\"audio, sr = librosa.load('path_to_audio.wav', sr=16000)\")\n",
    "print(\"plot_audio_analysis(audio, sr, title='Sample Audio')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad69f42",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db629502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MFCC extractor\n",
    "mfcc_extractor = MFCCExtractor(\n",
    "    sample_rate=config['data']['sample_rate'],\n",
    "    n_mfcc=config['features']['mfcc']['n_mfcc'],\n",
    "    n_fft=config['features']['mfcc']['n_fft'],\n",
    "    hop_length=config['features']['mfcc']['hop_length'],\n",
    "    use_deltas=config['features']['mfcc']['use_deltas'],\n",
    "    use_delta_deltas=config['features']['mfcc']['use_delta_deltas']\n",
    ")\n",
    "\n",
    "print(\"MFCC extractor initialized!\")\n",
    "print(f\"Number of MFCC coefficients: {mfcc_extractor.n_mfcc}\")\n",
    "print(f\"Using deltas: {mfcc_extractor.use_deltas}\")\n",
    "print(f\"Using delta-deltas: {mfcc_extractor.use_delta_deltas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample audio for demo\n",
    "sr = 16000\n",
    "duration = 3  # seconds\n",
    "sample_audio = np.random.randn(sr * duration) * 0.1\n",
    "\n",
    "# Extract MFCCs\n",
    "mfccs = mfcc_extractor.extract(sample_audio)\n",
    "\n",
    "print(f\"Sample audio shape: {sample_audio.shape}\")\n",
    "print(f\"Extracted MFCC features shape: {mfccs.shape}\")\n",
    "print(f\"Feature dimensions: {mfccs.shape[0]} coefficients × {mfccs.shape[1]} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b112868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MFCC features\n",
    "plt.figure(figsize=(14, 6))\n",
    "librosa.display.specshow(mfccs, sr=sr, hop_length=mfcc_extractor.hop_length, x_axis='time')\n",
    "plt.colorbar(format='%+2.0f')\n",
    "plt.title('MFCC Features (Demo)')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "plt.xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18836f",
   "metadata": {},
   "source": [
    "## 6. Data Splitting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test splits\n",
    "print(\"Creating data splits...\")\n",
    "splits = loader.create_splits(\n",
    "    train_size=config['data']['train_split'],\n",
    "    val_size=config['data']['val_split'],\n",
    "    test_size=config['data']['test_split'],\n",
    "    seed=config['seed']\n",
    ")\n",
    "\n",
    "print(\"\\nData Splits Created:\")\n",
    "print(f\"  Training samples:   {len(splits['train'])}\")\n",
    "print(f\"  Validation samples: {len(splits['val'])}\")\n",
    "print(f\"  Test samples:       {len(splits['test'])}\")\n",
    "print(f\"  Total:              {len(splits['train']) + len(splits['val']) + len(splits['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed692b",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "1. Dataset loaded successfully from HuggingFace\n",
    "2. Multiple Indian languages represented\n",
    "3. Both adult and child speakers (if applicable)\n",
    "4. Word and sentence level utterances (if applicable)\n",
    "5. MFCC feature extraction demonstrated\n",
    "\n",
    "### Next Steps:\n",
    "1. **Feature Extraction Notebook** (`02_feature_extraction.ipynb`):\n",
    "   - Extract MFCC features from all samples\n",
    "   - Extract HuBERT embeddings\n",
    "   - Save processed features\n",
    "\n",
    "2. **Model Training Notebook** (`03_training_and_eval.ipynb`):\n",
    "   - Train CNN, BiLSTM, and Transformer models\n",
    "   - Compare MFCC vs HuBERT performance\n",
    "   - Perform layer-wise analysis\n",
    "\n",
    "3. **Application Development**:\n",
    "   - Build cuisine recommendation system\n",
    "   - Test on real audio samples"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
